import sys
import os
import numpy as np
from tqdm import trange
from typing import Dict, List, Tuple, Optional
import cv2

import mujoco
sys.path.append('../exercise-yet-another-mujoco-tutorial-v3/package/helper/')
sys.path.append('../exercise-yet-another-mujoco-tutorial-v3/package/mujoco_usage/')
sys.path.append('../exercise-yet-another-mujoco-tutorial-v3/package/motion_retarget/')
from mujoco_parser import MuJoCoParserClass, init_ik_info, add_ik_info, get_dq_from_ik_info
from utility import *
from transformation import *
from joi import *
from mr_cmu import *
from scipy.spatial.transform import Rotation as R
import imageio


def ensure_T_joi_src_is_4x4(T_joi_src):
    # ì´ë¯¸ (N, 4, 4)ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if T_joi_src.ndim == 3 and T_joi_src.shape[1:] == (4, 4):
        return T_joi_src
    # (N, 3) ì¢Œí‘œë§Œ ìˆëŠ” ê²½ìš° â†’ 4x4 í–‰ë ¬ë¡œ ë³€í™˜ (íšŒì „ì€ ë‹¨ìœ„í–‰ë ¬)
    elif T_joi_src.ndim == 2 and T_joi_src.shape[1] == 3:
        N = T_joi_src.shape[0]
        T = np.tile(np.eye(4), (N, 1, 1))
        T[:, :3, 3] = T_joi_src
        return T
    # (N, 3, 3) íšŒì „í–‰ë ¬ë§Œ ìˆëŠ” ê²½ìš° â†’ 4x4ë¡œ ë³€í™˜
    elif T_joi_src.ndim == 3 and T_joi_src.shape[1:] == (3, 3):
        N = T_joi_src.shape[0]
        T = np.tile(np.eye(4), (N, 1, 1))
        T[:, :3, :3] = T_joi_src
        return T
    # (joints, features) í˜•íƒœì¸ ê²½ìš° â†’ (joints, 4, 4)ë¡œ ë³€í™˜
    elif T_joi_src.ndim == 2:
        N_joints = T_joi_src.shape[0]
        N_features = T_joi_src.shape[1]
        
        # ê° jointì— ëŒ€í•´ 4x4 ë³€í™˜ í–‰ë ¬ ìƒì„±
        T = np.tile(np.eye(4), (N_joints, 1, 1))
        
        # featuresë¥¼ 4x4 í–‰ë ¬ë¡œ ì¬êµ¬ì„±
        if N_features >= 16:
            # 16ê°œì”© ë¬¶ì–´ì„œ 4x4 í–‰ë ¬ë¡œ ë³€í™˜
            for i in range(N_joints):
                if i * 16 < N_features:
                    features = T_joi_src[i, :16]
                    T[i] = features.reshape(4, 4)
        elif N_features >= 12:
            # 12ê°œì”© ë¬¶ì–´ì„œ 3x4 í–‰ë ¬ë¡œ ë³€í™˜ í›„ 4x4ë¡œ í™•ì¥
            for i in range(N_joints):
                if i * 12 < N_features:
                    features = T_joi_src[i, :12]
                    T_3x4 = features.reshape(3, 4)
                    T[i, :3, :] = T_3x4
        elif N_features >= 6:
            # 6ê°œì”© ë¬¶ì–´ì„œ 3x3 íšŒì „í–‰ë ¬ë¡œ ë³€í™˜ í›„ 4x4ë¡œ í™•ì¥
            for i in range(N_joints):
                if i * 6 < N_features:
                    features = T_joi_src[i, :6]
                    R_3x3 = features.reshape(3, 3)
                    T[i, :3, :3] = R_3x3
        else:
            # ë‹¨ìˆœíˆ ìœ„ì¹˜ ì •ë³´ë§Œ ì‚¬ìš© (ì²« 3ê°œ featuresë¥¼ ìœ„ì¹˜ë¡œ)
            for i in range(N_joints):
                if N_features >= 3:
                    T[i, :3, 3] = T_joi_src[i, :3]
        return T
    else:
        raise ValueError(f"Unknown T_joi_src shape: {T_joi_src.shape}")


class MDMToHumanoidRetargeter():
    """MDM ëª¨ì…˜ì„ íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ìœ¼ë¡œ ë¦¬íƒ€ê²ŒíŒ…"""
    # super init
    def __init__(self, env: MuJoCoParserClass):
        self.env = env
        self.env.reset(step=True)
        self.smpl_to_g1_idx = {
            'hip': 0,      # pelvis
            'neck': 12,    # neck  
            'rs': 17,      # right_shoulder
            're': 19,      # right_elbow
            'rw': 21,      # right_wrist
            'ls': 16,      # left_shoulder
            'le': 18,      # left_elbow
            'lw': 20,      # left_wrist
            'rp': 2,       # right_hip
            'rk': 5,       # right_knee
            'ra': 8,       # right_ankle
            'lp': 1,       # left_hip
            'lk': 4,       # left_knee
            'la': 7        # left_ankle
        }
        self.width = 640
        self.height = 480
        self.camera_distance = 4


        self.free_cam = mujoco.MjvCamera()
        mujoco.mjv_defaultCamera(self.free_cam)
        self.free_cam.azimuth = 90      # ì¢Œìš° íšŒì „ (ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ì¡°ì ˆ)
        self.free_cam.elevation = -15   # ìœ„/ì•„ë˜ ê°ë„
        self.free_cam.distance = self.camera_distance

    def auto_scale_and_rotation(self, motion_data: np.ndarray, env: MuJoCoParserClass):
        source_height, source_orientation, source_center = self._calculate_source_dimensions(motion_data, ensure=True)
        target_height, target_orientation, target_center = self._calculate_target_dimensions(env)
        
        position_offset = target_center - source_center
        scale = target_height / source_height if source_height > 0 else 1.0
        scale = np.clip(scale, 0.1, 10.0)
        
        rotation = self._calculate_rotation_difference(source_orientation, target_orientation)
        rotation = tuple(np.clip(rot, -180, 180) for rot in rotation)
        
        print(f"ğŸ” ìë™ ìœ„ì¹˜/ìŠ¤ì¼€ì¼/íšŒì „ ê³„ì‚°:")
        print(f"   ì†ŒìŠ¤ ì¤‘ì‹¬: {source_center}, í‚¤: {source_height:.3f}m, ë°©í–¥: {source_orientation:.1f}Â°")
        print(f"   íƒ€ê²Ÿ ì¤‘ì‹¬: {target_center}, í‚¤: {target_height:.3f}m, ë°©í–¥: {target_orientation:.1f}Â°")
        
        return position_offset, scale, rotation
            
    
    def _calculate_source_dimensions(self, motion_data: np.ndarray, ensure: bool = False) -> Tuple[float, float, np.ndarray]:
        head_pos = motion_data[0, self.smpl_to_g1_idx['neck'], :3]
        left_foot_pos = motion_data[0, self.smpl_to_g1_idx['la'], :3]
        right_foot_pos = motion_data[0, self.smpl_to_g1_idx['ra'], :3]
        foot_center = (left_foot_pos + right_foot_pos) / 2
        center = (head_pos + foot_center) / 2
        
        height = np.linalg.norm(head_pos - foot_center)
        
        head_foot_vector = head_pos - foot_center
        horizontal_angle = np.degrees(np.arctan2(head_foot_vector[1], head_foot_vector[0]))
        
        foot_direction = right_foot_pos - left_foot_pos
        foot_angle = np.degrees(np.arctan2(foot_direction[1], foot_direction[0]))
        
        overall_orientation = (horizontal_angle + foot_angle) / 2
        
        return height, overall_orientation, center
    
    def _calculate_target_dimensions(self, env: MuJoCoParserClass) -> Tuple[float, float, np.ndarray]:
        """
        íƒ€ê²Ÿ ë¡œë´‡ì—ì„œ í‚¤, ë°©í–¥, ì¤‘ì‹¬ì  ê³„ì‚°
        
        Returns:
            height: ë¨¸ë¦¬ì—ì„œ ë°œê¹Œì§€ì˜ ê±°ë¦¬ (ë¯¸í„°)
            orientation: ë¨¸ë¦¬-ë°œ ë°©í–¥ ê°ë„ (ë„)
            center: ë¡œë´‡ì˜ ì¤‘ì‹¬ì  (x, y, z)
        """
        # ë¡œë´‡ì˜ í˜„ì¬ ê´€ì ˆ ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸°
        T_joi_trgt = get_T_joi_from_g1(env)
        
        # ë¨¸ë¦¬(neck)ì™€ ë°œ(ankle) ìœ„ì¹˜ ì¶”ì¶œ
        head_pos = t2p(T_joi_trgt['neck'])
        left_foot_pos = t2p(T_joi_trgt['la'])
        right_foot_pos = t2p(T_joi_trgt['ra'])
        
        # ë°œ ì¤‘ê°„ì  ê³„ì‚°
        foot_center = (left_foot_pos + right_foot_pos) / 2
        
        # ì¤‘ì‹¬ì  ê³„ì‚° (ë¨¸ë¦¬ì™€ ë°œì˜ ì¤‘ê°„ì )
        center = (head_pos + foot_center) / 2
        
        # í‚¤ ê³„ì‚° (ë¨¸ë¦¬ì—ì„œ ë°œê¹Œì§€ì˜ ê±°ë¦¬)
        height = np.linalg.norm(head_pos - foot_center)
        
        # ë°©í–¥ ê³„ì‚° (ë¨¸ë¦¬-ë°œ ë²¡í„°ì˜ ìˆ˜í‰ë©´ì—ì„œì˜ ê°ë„)
        head_foot_vector = head_pos - foot_center
        horizontal_angle = np.degrees(np.arctan2(head_foot_vector[1], head_foot_vector[0]))
        
        # ë°œê¿ˆì¹˜-ë°œì•ê¿ˆì¹˜ ë°©í–¥ë„ ê³ ë ¤ (ë°œì˜ ë°©í–¥)
        foot_direction = right_foot_pos - left_foot_pos
        foot_angle = np.degrees(np.arctan2(foot_direction[1], foot_direction[0]))
        
        # ì „ì²´ ë°©í–¥ì€ ë¨¸ë¦¬-ë°œ ë°©í–¥ê³¼ ë°œ ë°©í–¥ì˜ í‰ê· 
        overall_orientation = (horizontal_angle + foot_angle) / 2
        
        return height, overall_orientation, center
    
    def _calculate_rotation_difference(self, source_orientation: float, target_orientation: float) -> Tuple[float, float, float]:
        """
        ì†ŒìŠ¤ì™€ íƒ€ê²Ÿ ë°©í–¥ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬ íšŒì „ ê°ë„ ë°˜í™˜
        
        Args:
            source_orientation: ì†ŒìŠ¤ ë°©í–¥ ê°ë„ (ë„)
            target_orientation: íƒ€ê²Ÿ ë°©í–¥ ê°ë„ (ë„)
            
        Returns:
            rotation: (x, y, z) íšŒì „ ê°ë„ (ë„)
        """
        # Yì¶• íšŒì „ (ìˆ˜í‰ë©´ì—ì„œì˜ íšŒì „)
        y_rotation = target_orientation - source_orientation
        
        # ê°ë„ë¥¼ -180 ~ 180 ë²”ìœ„ë¡œ ì •ê·œí™”
        while y_rotation > 180:
            y_rotation -= 360
        while y_rotation < -180:
            y_rotation += 360
        
        # Xì¶•ê³¼ Zì¶• íšŒì „ì€ 0ìœ¼ë¡œ ì„¤ì • (í•„ìš”ì‹œ ì¶”ê°€ ê³„ì‚° ê°€ëŠ¥)
        x_rotation = 0.0
        z_rotation = 0.0
        
        return (x_rotation, y_rotation, z_rotation)

    def apply_scale_and_rotation(self, motion_data: np.ndarray, position_offset: np.ndarray = None, scale: float = None, rotation: Tuple[float, float, float] = None):
        if position_offset is None or scale is None or rotation is None:
            position_offset, scale, rotation = self.auto_scale_and_rotation(motion_data, self.env)
        
        rot_matrix = R.from_euler('xyz', rotation, degrees=True).as_matrix()
        
        if motion_data.ndim == 4:
            # (batch, joints, features, frames)
            for i in range(motion_data.shape[0]):
                for j in range(motion_data.shape[1]):
                    for k in range(motion_data.shape[3]):
                        # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ (ì²˜ìŒ 3ê°œ ìš”ì†Œê°€ ìœ„ì¹˜)
                        pos = motion_data[i, j, :3, k]
                        
                        # ë³€í™˜ ì ìš©: ìœ„ì¹˜ ì˜¤í”„ì…‹ + ìŠ¤ì¼€ì¼ + íšŒì „
                        pos = (pos * scale) + position_offset
                        pos = rot_matrix @ pos
                        
                        # ë³€í™˜ëœ ìœ„ì¹˜ë¥¼ ë‹¤ì‹œ ì €ì¥
                        motion_data[i, j, :3, k] = pos
                        
        elif motion_data.ndim == 3: # (frames, joints, features)
            for i in range(motion_data.shape[0]):
                for j in range(motion_data.shape[1]):
                    # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ (ì²˜ìŒ 3ê°œ ìš”ì†Œê°€ ìœ„ì¹˜)
                    pos = motion_data[i, j, :3]
                    
                    # ë³€í™˜ ì ìš©: ìœ„ì¹˜ ì˜¤í”„ì…‹ + ìŠ¤ì¼€ì¼ + íšŒì „
                    pos = (pos * scale) + position_offset
                    pos = rot_matrix @ pos
                    
                    # ë³€í™˜ëœ ìœ„ì¹˜ë¥¼ ë‹¤ì‹œ ì €ì¥
                    motion_data[i, j, :3] = pos
        else:
            raise ValueError(f"Unexpected motion data shape: {motion_data.shape}")
        
        return motion_data

    def retarget_g1_mujoco(
        self, 
        motion_data: np.ndarray, 
        update_base_every_frame: bool = False, 
        visualize: bool = False, 
        filename: str = 'save/retargeted_motion.mp4', 
        fps: int = 20,
        show_target_spheres: bool = True,
        position_offset: Optional[np.ndarray] = None,
        scale: Optional[float] = None,
        rotation: Optional[Tuple[float, float, float]] = None
        ):
        if self.env is None:
            raise ValueError("MuJoCo í™˜ê²½ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        motion_data = self.apply_scale_and_rotation(motion_data, position_offset, scale, rotation)
        
        qpos_list = []
        video_frames = []
        renderer = mujoco.Renderer(self.env.model, width=self.width, height=self.height)
        frames = motion_data.shape[0]
        target_spheres_list = []
        if visualize:
            self.env.init_viewer(title='MDM to G1 Motion Retargeting', transparent=True)
        
        tick = 0
        for tick in trange(frames):
            T_joi_src = motion_data[tick, :, :] 
            T_joi_src = ensure_T_joi_src_is_4x4(T_joi_src)
            T_joi_src = {k: T_joi_src[v, :, :] for k, v in self.smpl_to_g1_idx.items()}

            if update_base_every_frame or tick == 0:
                T_base_src  = T_joi_src['hip']
                T_base_trgt = T_yuzf2zuxf(T_base_src)
                
                self.env.set_T_base_body(body_name='pelvis',T=T_base_trgt)
                self.env.forward()
                # self.env.step()

            # Retargeting
            T_joi_trgt = get_T_joi_from_g1(self.env)
            len_hip2neck   = len_T_joi(T_joi_trgt,'hip','neck')
            len_neck2rs    = len_T_joi(T_joi_trgt,'neck','rs')
            len_rs2re      = len_T_joi(T_joi_trgt,'rs','re')
            len_re2rw      = len_T_joi(T_joi_trgt,'re','rw')
            len_neck2ls    = len_T_joi(T_joi_trgt,'neck','ls')
            len_ls2le      = len_T_joi(T_joi_trgt,'ls','le')
            len_le2lw      = len_T_joi(T_joi_trgt,'le','lw')
            len_hip2rp     = len_T_joi(T_joi_trgt,'hip','rp')
            len_rp2rk      = len_T_joi(T_joi_trgt,'rp','rk')
            len_rk2ra      = len_T_joi(T_joi_trgt,'rk','ra')
            len_hip2lp     = len_T_joi(T_joi_trgt,'hip','lp')
            len_lp2lk      = len_T_joi(T_joi_trgt,'lp','lk')
            len_lk2la      = len_T_joi(T_joi_trgt,'lk','la')
            
            rev_joint_names_for_ik_full_body = self.env.rev_joint_names
            joint_idxs_jac_full_body = self.env.get_idxs_jac(
                joint_names=rev_joint_names_for_ik_full_body)
            joint_idxs_jac_full_body_with_base = np.concatenate(
                ([0,1,2,3,4,5],joint_idxs_jac_full_body))

            uv_hip2neck   = uv_T_joi(T_joi_src,'hip','neck')
            uv_neck2rs    = uv_T_joi(T_joi_src,'neck','rs')
            uv_rs2re      = uv_T_joi(T_joi_src,'rs','re')
            uv_re2rw      = uv_T_joi(T_joi_src,'re','rw')
            uv_neck2ls    = uv_T_joi(T_joi_src,'neck','ls')
            uv_ls2le      = uv_T_joi(T_joi_src,'ls','le')
            uv_le2lw      = uv_T_joi(T_joi_src,'le','lw')
            uv_hip2rp     = uv_T_joi(T_joi_src,'hip','rp')
            uv_rp2rk      = uv_T_joi(T_joi_src,'rp','rk')
            uv_rk2ra      = uv_T_joi(T_joi_src,'rk','ra')
            uv_hip2lp     = uv_T_joi(T_joi_src,'hip','lp')
            uv_lp2lk      = uv_T_joi(T_joi_src,'lp','lk')
            uv_lk2la      = uv_T_joi(T_joi_src,'lk','la')
            
            # Set positional targets with robust calculation
            try:
                if update_base_every_frame:
                    T_joi_trgt_current = get_T_joi_from_g1(self.env)
                    p_hip_current = t2p(T_joi_trgt_current['hip'])
                    p_hip_trgt = p_hip_current
                else:
                    p_hip_trgt = t2p(T_joi_src['hip'])
                
                p_neck_trgt  = p_hip_trgt + len_hip2neck*uv_hip2neck
                p_rs_trgt    = p_neck_trgt + len_neck2rs*uv_neck2rs
                p_re_trgt    = p_rs_trgt + len_rs2re*uv_rs2re
                p_rw_trgt    = p_re_trgt + len_re2rw*uv_re2rw
                p_ls_trgt    = p_neck_trgt + len_neck2ls*uv_neck2ls
                p_le_trgt    = p_ls_trgt + len_ls2le*uv_ls2le
                p_lw_trgt    = p_le_trgt + len_le2lw*uv_le2lw
                p_rp_trgt    = p_hip_trgt + len_hip2rp*uv_hip2rp
                p_rk_trgt    = p_rp_trgt + len_rp2rk*uv_rp2rk
                p_ra_trgt    = p_rk_trgt + len_rk2ra*uv_rk2ra
                p_lp_trgt    = p_hip_trgt + len_hip2lp*uv_hip2lp
                p_lk_trgt    = p_lp_trgt + len_lp2lk*uv_lp2lk
                p_la_trgt    = p_lk_trgt + len_lk2la*uv_lk2la
                
                target_positions = [p_neck_trgt, p_rs_trgt, p_re_trgt, p_rw_trgt, 
                                    p_ls_trgt, p_le_trgt, p_lw_trgt, p_rp_trgt, 
                                    p_rk_trgt, p_ra_trgt, p_lp_trgt, p_lk_trgt, p_la_trgt]
                target_spheres_list = {
                    'hip': p_hip_trgt,
                    'neck': p_neck_trgt,
                    'rs': p_rs_trgt,
                    're': p_re_trgt,
                    'rw': p_rw_trgt,
                    'ls': p_ls_trgt,
                    'le': p_le_trgt,
                    'lw': p_lw_trgt,
                    'rp': p_rp_trgt,
                    'rk': p_rk_trgt,
                    'ra': p_ra_trgt,
                    'lp': p_lp_trgt,
                    'lk': p_lk_trgt,
                    'la': p_la_trgt
                }
                
                invalid_positions = False
                for i, pos in enumerate(target_positions):
                    if np.any(np.isnan(pos)) or np.any(np.isinf(pos)):
                        print(f"Warning: Invalid target position {i} at tick {tick}, skipping frame")
                        invalid_positions = True
                        break
                
                if invalid_positions:
                    continue
                        
            except Exception as e:
                print(f"Error calculating target positions at tick {tick}: {e}")
                continue

            joi_body_name = get_joi_body_name_of_g1()
            ik_info_full_body = init_ik_info()
            add_ik_info(ik_info_full_body,body_name=joi_body_name['rs'],p_trgt=p_rs_trgt)
            add_ik_info(ik_info_full_body,body_name=joi_body_name['re'],p_trgt=p_re_trgt)
            add_ik_info(ik_info_full_body,body_name=joi_body_name['rw'],p_trgt=p_rw_trgt)
            add_ik_info(ik_info_full_body,body_name=joi_body_name['ls'],p_trgt=p_ls_trgt)
            add_ik_info(ik_info_full_body,body_name=joi_body_name['le'],p_trgt=p_le_trgt)
            add_ik_info(ik_info_full_body,body_name=joi_body_name['lw'],p_trgt=p_lw_trgt)
            add_ik_info(ik_info_full_body,body_name=joi_body_name['rp'],p_trgt=p_rp_trgt)
            add_ik_info(ik_info_full_body,body_name=joi_body_name['rk'],p_trgt=p_rk_trgt)
            add_ik_info(ik_info_full_body,body_name=joi_body_name['ra'],p_trgt=p_ra_trgt)
            add_ik_info(ik_info_full_body,body_name=joi_body_name['lp'],p_trgt=p_lp_trgt)
            add_ik_info(ik_info_full_body,body_name=joi_body_name['lk'],p_trgt=p_lk_trgt)
            add_ik_info(ik_info_full_body,body_name=joi_body_name['la'],p_trgt=p_la_trgt)

            max_ik_tick = 50
            ik_converged = False
            
            for ik_tick in range(max_ik_tick):
                dq,ik_err_stack = get_dq_from_ik_info(
                    env            = self.env,
                    ik_info        = ik_info_full_body,
                    stepsize       = 0.8,
                    eps            = 1e-2,
                    th             = np.radians(10.0),
                    joint_idxs_jac = joint_idxs_jac_full_body_with_base,
                )
                
                if np.any(np.isnan(dq)) or np.any(np.isinf(dq)):
                    print(f"Warning: Invalid dq at tick {tick}, ik_tick {ik_tick}")
                    break
                    
                qpos = self.env.get_qpos()
                qpos_backup = qpos.copy()
                
                mujoco.mj_integratePos(self.env.model, qpos, dq, 1)
                
                if np.any(np.isnan(qpos)) or np.any(np.isinf(qpos)):
                    print(f"Warning: Invalid qpos after integration at tick {tick}")
                    qpos = qpos_backup
                    break
                    
                self.env.forward(q=qpos)
                # self.env.step(ctrl=qpos, joint_names=self.env.joint_names)
                        
                # except Exception as e:
                #     print(f"Error in mj_integratePos at tick {tick}: {e}")
                #     qpos = qpos_backup
                #     break
                
                if np.linalg.norm(ik_err_stack) < 0.05:
                    ik_converged = True
                    break
                        
            status = "converged" if ik_converged else "failed"
            print(f"Tick {tick}: IK {status} after {ik_tick+1}/{max_ik_tick} iterations, Error: {np.linalg.norm(ik_err_stack):.4f}")
            qpos_list.append(self.env.get_qpos())
            video_frame = self.grab_image(target_spheres_list, renderer, show_target_spheres=show_target_spheres)
            video_frames.append(video_frame)
            
        if visualize:
            self.env.close_viewer()
            
        imageio.mimsave(filename, video_frames, fps=fps)
        print(f"[MuJoCo] mp4 ì €ì¥ ì™„ë£Œ (frames: {len(video_frames)}): {filename}")
        print ("Done.")
    
    def grab_image(self, target_spheres: Dict[str, np.ndarray], renderer: mujoco.Renderer, show_target_spheres: bool = True) -> np.ndarray:
        mujoco.mj_forward(self.env.model, self.env.data)

        # pelvis ìœ„ì¹˜ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì¹´ë©”ë¼ ìœ„ì¹˜ ì—…ë°ì´íŠ¸
        pelvis_id = self.env.model.body('pelvis').id
        pelvis_pos = self.env.data.xpos[pelvis_id]
        self.free_cam.lookat[:] = pelvis_pos
        self.free_cam.distance = self.camera_distance  # í•„ìš”í•˜ë©´ ë§¤ í”„ë ˆì„ ë‹¤ë¥¸ ê°’ë„ ê°€ëŠ¥

        # free camera ì‚¬ìš©
        renderer.update_scene(self.env.data, camera=self.free_cam)
        frame = renderer.render()
        color_map = {
            k: (128, 128, 128) for k in target_spheres.keys() # gray
        }
        color_map['la'] = (0, 0, 0) # black
        color_map['ra'] = (0, 255, 0) # green
        color_map['hip'] = (255, 0, 0) # red
        color_map['neck'] = (0, 0, 255) # blue

        if show_target_spheres:
            cx, cy = self.width // 2, self.height // 2
            scale = 80.0
            for k, p in target_spheres.items():
                u = int(cx + p[0] * scale)
                v = int(cy - p[1] * scale)
                frame = cv2.circle(frame, (u, v), 5, color_map[k], -1)

        return frame


def retarget_motion(
    # motion_file = './save/humanml_enc_512_50steps/samples_humanml_enc_512_50steps_000750000_seed10_a_person_walks_forward/results.npy',
    motion_file = '/scratch2/iyy1112/motion-persona/save/20250805_mdm_type3/gpu_4/motion_1850_rep_00/motion.npy',
    output_file = './save/retargeted_motion.mp4',
    # output_file = '/scratch2/iyy1112/motion-persona/save/20250805_mdm_type3/gpu_4/motion_1849_rep_00/retarget_motion_1849_rep_00.mp4',
    xml_path = '../exercise-yet-another-mujoco-tutorial-v3/asset/unitree_g1/scene_g1.xml',
    # xml_path = '../exercise-yet-another-mujoco-tutorial-v3/asset/unitree_g1/g1.xml',
    position_offset = None,
    scale = None,
    rotation = None, # (89.7, 0, 0)
    show_target_spheres = True
):
    if not os.path.exists(motion_file):
        raise FileNotFoundError(f"ğŸ’¡ ëª¨ì…˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {motion_file}")
    else:
        data = np.load(motion_file, allow_pickle=True).item()
        
        # Handle both individual motion files and combined results files
        if 'motion' in data and isinstance(data['motion'], dict):
            # Individual motion file (from generate.py)
            motion_data = data['motion']['motion']
            print(f"ğŸ¯ ê°œë³„ ëª¨ì…˜ íŒŒì¼ì—ì„œ ë¡œë“œ: {motion_data.shape}")
        elif 'motion' in data and isinstance(data['motion'], np.ndarray):
            # Combined results file
            motion_data = data['motion']
            print(f"ğŸ¯ í†µí•© ê²°ê³¼ íŒŒì¼ì—ì„œ ë¡œë“œ: {motion_data.shape}")
        else:
            # Direct motion data
            motion_data = data
            print(f"ğŸ¯ ì§ì ‘ ëª¨ì…˜ ë°ì´í„°: {motion_data.shape}")

    env = MuJoCoParserClass(name='G1_tmp', rel_xml_path=xml_path, verbose=False)
    retargeter = MDMToHumanoidRetargeter(env)
    retargeter.retarget_g1_mujoco(
        motion_data, 
        filename=output_file, 
        show_target_spheres=show_target_spheres, 
        position_offset=position_offset,
        scale=scale, 
        rotation=rotation
    )
    print("\nğŸ‰ G1 ë¦¬íƒ€ê²ŒíŒ… ì™„ë£Œ!")


def meta_run(
    motions = '../ex-MoMo/save/20250724_1224',
):
    motion_dirs = os.listdir(motions)
    for motion_dir in motion_dirs:
        motion_file = os.path.join(motions, motion_dir, 'results.npy')
        retarget_motion(motion_file=motion_file, output_file=os.path.join(motions, motion_dir, f'{motion_dir}.mp4'))


if __name__ == "__main__":
    import fire
    fire.Fire(retarget_motion)